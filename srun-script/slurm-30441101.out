Head node is: v006
Starting Ray worker on v006.ib.bridges2.psc.edu, connecting to head node v006
2025-04-16 02:23:17,829	INFO scripts.py:1047 -- Local node IP: 10.8.2.255
DEBUG 04-16 02:35:44 [__init__.py:28] No plugins for group vllm.platform_plugins found.
DEBUG 04-16 02:35:44 [__init__.py:34] Checking if TPU platform is available.
DEBUG 04-16 02:35:44 [__init__.py:44] TPU platform is not available because: No module named 'libtpu'
DEBUG 04-16 02:35:44 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 04-16 02:35:44 [__init__.py:72] Confirmed CUDA platform is available.
DEBUG 04-16 02:35:44 [__init__.py:100] Checking if ROCm platform is available.
DEBUG 04-16 02:35:44 [__init__.py:114] ROCm platform is not available because: No module named 'amdsmi'
DEBUG 04-16 02:35:44 [__init__.py:122] Checking if HPU platform is available.
DEBUG 04-16 02:35:44 [__init__.py:129] HPU platform is not available because habana_frameworks is not found.
DEBUG 04-16 02:35:44 [__init__.py:140] Checking if XPU platform is available.
DEBUG 04-16 02:35:44 [__init__.py:150] XPU platform is not available because: No module named 'intel_extension_for_pytorch'
DEBUG 04-16 02:35:44 [__init__.py:158] Checking if CPU platform is available.
DEBUG 04-16 02:35:44 [__init__.py:180] Checking if Neuron platform is available.
DEBUG 04-16 02:35:44 [__init__.py:187] Neuron platform is not available because: No module named 'transformers_neuronx'
DEBUG 04-16 02:35:44 [__init__.py:52] Checking if CUDA platform is available.
DEBUG 04-16 02:35:44 [__init__.py:72] Confirmed CUDA platform is available.
INFO 04-16 02:35:44 [__init__.py:239] Automatically detected platform cuda.
DEBUG 04-16 02:35:47 [__init__.py:28] No plugins for group vllm.general_plugins found.
INFO 04-16 02:35:55 [config.py:600] This model supports multiple tasks: {'score', 'generate', 'reward', 'embed', 'classify'}. Defaulting to 'generate'.
WARNING 04-16 02:35:55 [arg_utils.py:1708] Compute Capability < 8.0 is not supported by the V1 Engine. Falling back to V0. 
INFO 04-16 02:35:55 [llm_engine.py:242] Initializing a V0 LLM engine (v0.8.3) with config: model='facebook/opt-66b', speculative_config=None, tokenizer='facebook/opt-66b', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.float16, max_seq_len=2048, download_dir=None, load_format=auto, tensor_parallel_size=8, pipeline_parallel_size=2, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar', reasoning_backend=None), observability_config=ObservabilityConfig(show_hidden_metrics=False, otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=None, served_model_name=facebook/opt-66b, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=None, chunked_prefill_enabled=False, use_async_output_proc=False, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"splitting_ops":[],"compile_sizes":[],"cudagraph_capture_sizes":[256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":256}, use_cached_outputs=False, 
INFO 04-16 02:35:59 [ray_utils.py:335] No current placement group found. Creating a new placement group.
WARNING 04-16 02:35:59 [ray_utils.py:342] The number of required GPUs exceeds the total number of available GPUs in the placement group.
INFO 04-16 02:36:09 [ray_utils.py:233] Waiting for creating a placement group of specs for 10 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 04-16 02:36:29 [ray_utils.py:233] Waiting for creating a placement group of specs for 30 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 04-16 02:37:09 [ray_utils.py:233] Waiting for creating a placement group of specs for 70 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 04-16 02:38:29 [ray_utils.py:233] Waiting for creating a placement group of specs for 150 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 04-16 02:41:09 [ray_utils.py:233] Waiting for creating a placement group of specs for 310 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 04-16 02:46:29 [ray_utils.py:233] Waiting for creating a placement group of specs for 630 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 04-16 02:57:09 [ray_utils.py:233] Waiting for creating a placement group of specs for 1270 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
INFO 04-16 03:18:29 [ray_utils.py:233] Waiting for creating a placement group of specs for 2550 seconds. specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}]. Check `ray status` and `ray list nodes` to see if you have enough resources, and make sure the IP addresses used by ray cluster are the same as VLLM_HOST_IP environment variable specified in each node if you are running on a multi-node.
Error: Cannot provide a placement group of placement_group_specs=[{'GPU': 1.0, 'node:10.8.10.255': 0.001}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}, {'GPU': 1.0}] within 1800 seconds. See `ray status` and `ray list nodes` to make sure the cluster has enough resources.
INFO 04-16 03:18:29 [ray_distributed_executor.py:127] Shutting down Ray distributed executor. If you see error log from logging.cc regarding SIGTERM received, please ignore because this is the expected termination process in Ray.
